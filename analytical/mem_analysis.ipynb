{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama2 7B activations, s=4096 ==============>\n",
      "QKV input size: 0.03125 GB\n",
      "Q size: 0.03125 GB\n",
      "K size: 0.03125 GB\n",
      "V size: 0.03125 GB\n",
      "O size: 0.03125 GB\n",
      "Softmax output size: 1.0 GB\n",
      "RMSNorm input size: 0.0625 GB\n",
      "MLP up input size: 0.03125 GB\n",
      "MLP up output size: 0.083984375 GB\n",
      "MLP gate output size: 0.083984375 GB\n",
      "MLP down input size: 0.083984375 GB\n",
      "Activation size: 1.501953125 GB\n",
      "Activation size: 48.0625 GB\n",
      "llama2 7B activations, s=512 ==============>\n",
      "QKV input size: 0.00390625 GB\n",
      "Q size: 0.00390625 GB\n",
      "K size: 0.00390625 GB\n",
      "V size: 0.00390625 GB\n",
      "O size: 0.00390625 GB\n",
      "Softmax output size: 0.015625 GB\n",
      "RMSNorm input size: 0.0078125 GB\n",
      "MLP up input size: 0.00390625 GB\n",
      "MLP up output size: 0.010498046875 GB\n",
      "MLP gate output size: 0.010498046875 GB\n",
      "MLP down input size: 0.010498046875 GB\n",
      "Activation size: 0.078369140625 GB\n",
      "Activation size: 2.5078125 GB\n",
      "llama2 70B activations==============>\n",
      "QKV input size: 0.0625 GB\n",
      "Q size: 0.0625 GB\n",
      "K size: 0.0078125 GB\n",
      "V size: 0.0078125 GB\n",
      "O size: 0.0625 GB\n",
      "Softmax output size: 2.0 GB\n",
      "RMSNorm input size: 0.125 GB\n",
      "MLP up input size: 0.0625 GB\n",
      "MLP up output size: 0.21875 GB\n",
      "MLP gate output size: 0.21875 GB\n",
      "MLP down input size: 0.21875 GB\n",
      "Activation size: 3.046875 GB\n",
      "Activation size: 243.75 GB\n",
      "Qwen2 7B activations==============>\n",
      "QKV input size: 0.02734375 GB\n",
      "Q size: 0.02734375 GB\n",
      "K size: 0.00390625 GB\n",
      "V size: 0.00390625 GB\n",
      "O size: 0.02734375 GB\n",
      "Softmax output size: 0.875 GB\n",
      "RMSNorm input size: 0.0546875 GB\n",
      "MLP up input size: 0.02734375 GB\n",
      "MLP up output size: 0.14453125 GB\n",
      "MLP gate output size: 0.14453125 GB\n",
      "MLP down input size: 0.14453125 GB\n",
      "Activation size: 1.48046875 GB\n",
      "Activation size: 41.453125 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41.453125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing\n",
    "GB = 2**30\n",
    "TOPS = 1e12\n",
    "C2C = 200/8  * GB  # per s\n",
    "head_num = 32 # attention heads\n",
    "kv_head = 32 # kv heads\n",
    "b = 1 # batchsize\n",
    "s = 4096 # sequnece length\n",
    "h = 4096 # hidden size\n",
    "immediate_size = 11008 # intermediate size\n",
    "layer_num = 32\n",
    "def llama_activation(head_num, kv_head, b, s, h, immediate_size, layer_num, is_print=True):\n",
    "    qkv_input = 2* b* s* h\n",
    "    q = 2* b * s * h\n",
    "    k = 2 * b * s * h/head_num * kv_head\n",
    "    v = 2 * b * s * h/head_num * kv_head\n",
    "    o = 2 * b * s * h\n",
    "    softmax_output = 2 * b * head_num * s * s\n",
    "    norm_input = 2 * b * s * h * 2\n",
    "    mlp_up_input = 2 * b * s * h\n",
    "    mlp_up_output = 2 * b * s * immediate_size\n",
    "    mlp_gate_output = 2 * b * s * immediate_size\n",
    "    mlp_down_input = 2 * b * s * immediate_size\n",
    "    activation = qkv_input + q + k + v + o + softmax_output + norm_input + mlp_up_input + \\\n",
    "        mlp_up_output + mlp_gate_output + mlp_down_input\n",
    "    if is_print:\n",
    "        print(f\"QKV input size: {qkv_input/GB} GB\")\n",
    "        print(f\"Q size: {q/GB} GB\")\n",
    "        print(f\"K size: {k/GB} GB\")\n",
    "        print(f\"V size: {v/GB} GB\")\n",
    "        print(f\"O size: {o/GB} GB\")\n",
    "        print(f\"Softmax output size: {softmax_output/GB} GB\")\n",
    "        print(f\"RMSNorm input size: {norm_input/GB} GB\")\n",
    "        print(f\"MLP up input size: {mlp_up_input/GB} GB\")\n",
    "        print(f\"MLP up output size: {mlp_up_output/GB} GB\")\n",
    "        print(f\"MLP gate output size: {mlp_gate_output/GB} GB\")\n",
    "        print(f\"MLP down input size: {mlp_down_input/GB} GB\")\n",
    "\n",
    "        print(f\"Activation size: {activation/GB} GB\")\n",
    "        print(f\"Activation size: {activation*layer_num/GB} GB\")\n",
    "    return activation*layer_num/GB\n",
    "print('llama2 7B activations, s=4096 ==============>')\n",
    "llama_activation(head_num=32, kv_head=32, b=1, s=4096, h=4096, immediate_size=11008, layer_num=32)\n",
    "print('llama2 7B activations, s=512 ==============>')\n",
    "llama_activation(head_num=32, kv_head=32, b=1, s=512, h=4096, immediate_size=11008, layer_num=32)\n",
    "print('llama2 70B activations==============>')\n",
    "llama_activation(head_num=64, kv_head=8, b=1, s=4096, h=8192, immediate_size=28672, layer_num=80)\n",
    "print('Qwen2 7B activations==============>')\n",
    "llama_activation(head_num=28, kv_head=4, b=1, s=4096, h=3584, immediate_size=18944, layer_num=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def megatron_version(a, b, s, h, L, t=1):\n",
    "    bytes = s*b*h*L*(10 + 24/t + 5*a*s/h/t)\n",
    "    return bytes/GB\n",
    "\n",
    "megatron_version(a=32, b=1, s=4096, h=4096, L=32, t=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2 7B==============>\n",
      "Weights: 14.193356037139893 GB\n",
      "Activations: 2.501953125 GB\n",
      "Grad: 0.3212890625 GB\n",
      "Lora Weights: 0.3212890625 GB\n",
      "Optimizer: 1.927734375 GB\n",
      "Total: 19.265621662139893 GB\n"
     ]
    }
   ],
   "source": [
    "print('Qwen2 7B==============>')\n",
    "params_num = 7.62 * 10**9\n",
    "weights = 2 * params_num/GB\n",
    "activations = llama_activation(head_num=28, kv_head=4, b=1, s=512, h=3584, immediate_size=18944, layer_num=28, is_print=False)\n",
    "\n",
    "lora_r = 64\n",
    "hidden_size = 3584\n",
    "immediate_size = 18944\n",
    "layer_num = 28\n",
    "lora_A_q_k_v_o = hidden_size * lora_r * 2\n",
    "lora_B_q_o =  lora_r * hidden_size * 2\n",
    "lora_B_k_v = lora_r * hidden_size/head_num*kv_head * 2\n",
    "lora_A_up_gate = hidden_size * lora_r * 2\n",
    "lora_B_up_gate = lora_r * immediate_size * 2\n",
    "lora_A_down = immediate_size * lora_r * 2\n",
    "lora_B_down = lora_r * hidden_size * 2\n",
    "lora_weights = layer_num*(4 * lora_A_q_k_v_o + 2 * lora_B_q_o + 2 * lora_B_k_v + 3 * lora_A_up_gate + 3 * lora_B_up_gate)/GB\n",
    "grad = lora_weights\n",
    "optimizer = lora_weights/2*12\n",
    "print(f'Weights: {weights} GB')\n",
    "print(f'Activations: {activations} GB')\n",
    "print(f'Grad: {grad} GB')\n",
    "print(f'Lora Weights: {lora_weights} GB')\n",
    "print(f'Optimizer: {optimizer} GB')\n",
    "print(f'Total: {weights + activations + grad + lora_weights + optimizer} GB') \n",
    "\n",
    "# batchsize = 1->2; 28GB -> 34GB  实测\n",
    "# 脚本，却是 19->21GB,只增加2.5GB\n",
    "\n",
    "# 用了flash att是 58GB\n",
    "# SPDA 是 60多GB\n",
    "# 不用 是 OutOfMem\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "length = 512\n",
    "\n",
    "512*3584  3584,3584\n",
    "\n",
    "\n",
    "4096*4096*2  = 32MB\n",
    "32/2 = 16 MB\n",
    "\n",
    "4096, 2048，  4096， 128\n",
    "\n",
    "3584/2 = 1792\n",
    "\n",
    "3584, 1792   3584, 112\n",
    "\n",
    "\n",
    "计算量统计\n",
    "读写量统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchsize = 1 >>>>>>>>>>>>>>>\n",
      "TP, Comm bytes: 0.0625 GB\n",
      "TP, Comm bytes total: 2.0 GB\n",
      "TP, Comm time: 0.08 s\n",
      "batchsize = 4 >>>>>>>>>>>>>>>\n",
      "TP, Comm bytes: 0.25 GB\n",
      "TP, Comm bytes total: 8.0 GB\n",
      "TP, Comm time: 0.32 s\n"
     ]
    }
   ],
   "source": [
    "def all_reduce(C2C, bytes, device_num):\n",
    "    comm_time = bytes/device_num/C2C * (device_num-1) * 2\n",
    "    return comm_time\n",
    "\n",
    "b , s, h, immediate_size = 1, 4096, 4096, 11008\n",
    "\n",
    "layer_num = 32\n",
    "tp = 2\n",
    "def TP_comm(b, s, h, layer_num):\n",
    "    print(f'batchsize = {b} >>>>>>>>>>>>>>>')\n",
    "    comm_byte = b * s * h * 2 + b * s * h * 2\n",
    "    print(f\"TP, Comm bytes: {comm_byte/GB} GB\")\n",
    "    comm_byte_total = comm_byte * layer_num\n",
    "    print(f\"TP, Comm bytes total: {comm_byte_total/GB} GB\")\n",
    "    comm_time = all_reduce(C2C, comm_byte_total, tp)\n",
    "    print(f\"TP, Comm time: {comm_time} s\")\n",
    "\n",
    "TP_comm(b, s, h, layer_num)\n",
    "TP_comm(4, s, h, layer_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP Comm bytes: 7.096678018569946 GB\n",
      "DP Comm time: 0.532250851392746 s\n"
     ]
    }
   ],
   "source": [
    "params_num = 7.62 * 10**9\n",
    "weights = 2 * params_num\n",
    "tp = 2\n",
    "total_devcie = 32\n",
    "comm_byte = weights/tp\n",
    "print(f\"DP Comm bytes: {comm_byte/GB} GB\")\n",
    "comm_time = all_reduce(C2C, comm_byte, total_devcie/tp)\n",
    "print(f\"DP Comm time: {comm_time} s\")\n",
    "\n",
    "#DP 时间不会随着batchszie变，但是会随着tp变\n",
    "# 所以增加tp，减少DP时间，同时增大batchsize，是个不错的选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora, DP Comm bytes: 0.3212890625 GB\n",
      "lora, DP Comm time: 0.02489990234375 s\n"
     ]
    }
   ],
   "source": [
    "print(f\"lora, DP Comm bytes: {lora_weights} GB\")\n",
    "comm_time = all_reduce(C2C, lora_weights*GB, total_devcie)\n",
    "print(f\"lora, DP Comm time: {comm_time} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total compute: 31.21152 TOPS\n",
      "Compute time: 0.24384 s\n",
      "batchsize = 4 >>>>>>>>>>>>>>>\n",
      "TP, Comm bytes: 0.03125 GB\n",
      "TP, Comm bytes total: 1.0 GB\n",
      "TP, Comm time: 0.04 s\n"
     ]
    }
   ],
   "source": [
    "compute_per_token = 2 * params_num\n",
    "s = 512\n",
    "b  = 4\n",
    "total_compute = compute_per_token * s * b\n",
    "hw_compute = 128 * TOPS\n",
    "utilization = 1\n",
    "print(f\"Total compute: {total_compute/TOPS} TOPS\")\n",
    "print(f\"Compute time: {total_compute/hw_compute/utilization} s\")\n",
    "\n",
    "TP_comm(b, s, h, layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total compute: 62.42304 TOPS\n",
      "Compute time: 0.48768 s\n"
     ]
    }
   ],
   "source": [
    "s = 4096\n",
    "total_compute = compute_per_token * s\n",
    "print(f\"Total compute: {total_compute/TOPS} TOPS\")\n",
    "print(f\"Compute time: {total_compute/hw_compute} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算die\n",
    "# 24*32\n",
    "\n",
    "56*2*256/8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409.6\n"
     ]
    }
   ],
   "source": [
    "m, k, n = 512, 4096, 4096\n",
    "\n",
    "operation_intensity = 2 * m * n * k / (2 * m * n + 2* m * k + 2*k * n)\n",
    "print(operation_intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama2-7B ===============>>>>>>>>>>>>>>>\n",
      "q_proj:  13.1941 TOPS\n",
      "k_proj:  13.1941 TOPS\n",
      "v_proj:  13.1941 TOPS\n",
      "att_q_k:  13.1941 TOPS\n",
      "att_v:  13.1941 TOPS\n",
      "o_proj:  13.1941 TOPS\n",
      "mlp_up:  35.4592 TOPS\n",
      "mlp_down:  35.4592 TOPS\n",
      "mlp_gate:  35.4592 TOPS\n",
      "lm_head:  3.2212 TOPS\n",
      "total:  188.7638 TOPS\n",
      "Qwe2-7B ===============>>>>>>>>>>>>>>>\n",
      "q_proj:  8.8390 TOPS\n",
      "k_proj:  1.2627 TOPS\n",
      "v_proj:  1.2627 TOPS\n",
      "att_q_k:  10.1018 TOPS\n",
      "att_v:  10.1018 TOPS\n",
      "o_proj:  8.8390 TOPS\n",
      "mlp_up:  46.7207 TOPS\n",
      "mlp_down:  46.7207 TOPS\n",
      "mlp_gate:  46.7207 TOPS\n",
      "lm_head:  13.3939 TOPS\n",
      "total:  193.9629 TOPS\n",
      "Qwe2-7B ===============>>>>>>>>>>>>>>>\n",
      "q_proj:  2.2098 TOPS\n",
      "k_proj:  0.3157 TOPS\n",
      "v_proj:  0.3157 TOPS\n",
      "att_q_k:  0.3157 TOPS\n",
      "att_v:  0.3157 TOPS\n",
      "o_proj:  2.2098 TOPS\n",
      "mlp_up:  11.6802 TOPS\n",
      "mlp_down:  11.6802 TOPS\n",
      "mlp_gate:  11.6802 TOPS\n",
      "lm_head:  3.3485 TOPS\n",
      "total:  44.0712 TOPS\n"
     ]
    }
   ],
   "source": [
    "def analyze_computation(batch_size, seq_len, vocab_size, hidden_size, head_num, kv_head, immediate_size, layer_num, mlp_style='llama'):\n",
    "    '''\n",
    "    count and analyze computations of LLM model training\n",
    "    Reference:\n",
    "    'Efficient large-scale language model training on GPU clusters using megatron-LM' \n",
    "    https://dl.acm.org/doi/10.1145/3458817.3476209, page11, APPENDIX: FLOATING-POINT OPERATIONS\n",
    "    \n",
    "    Parameters:\n",
    "    - batch_size (int): The batch size.\n",
    "    - seq_len (int): The sequence length.\n",
    "    - vocab_size (int): The vocabulary size.\n",
    "    - hidden_size (int): The hidden size.\n",
    "    - head_num (int): The number of attention heads.\n",
    "    - kv_head (int): The number of key-value heads.\n",
    "    - immediate_size (int): The immediate size, ffn up dim.\n",
    "    - layer_num (int): The number of layers.\n",
    "    - mlp_style (str): The style of mlp, 'llama' or 'normal'.\n",
    "\n",
    "    Returns:\n",
    "    - int: The total computations (FLOPs).\n",
    "    '''\n",
    "    compuations = {}\n",
    "    # forward pass\n",
    "    compuations['q_proj'] = batch_size * 2 * seq_len * hidden_size * hidden_size\n",
    "    compuations['k_proj'] = batch_size * 2 * seq_len * hidden_size * (hidden_size // head_num * kv_head)\n",
    "    compuations['v_proj'] = batch_size * 2 * seq_len * hidden_size * (hidden_size // head_num * kv_head)\n",
    "    compuations['att_q_k'] = batch_size * 2 *  seq_len * seq_len * hidden_size\n",
    "    compuations['att_v'] = batch_size * 2 * seq_len * seq_len * hidden_size\n",
    "    compuations['o_proj'] = batch_size * 2 * seq_len * hidden_size * hidden_size\n",
    "    compuations['mlp_up'] = batch_size * 2 * seq_len * hidden_size * immediate_size\n",
    "    compuations['mlp_down'] = batch_size * 2 * seq_len * immediate_size * hidden_size\n",
    "    \n",
    "    if mlp_style == 'llama':        \n",
    "        compuations['mlp_gate'] = batch_size * 2 * seq_len * hidden_size * immediate_size\n",
    "    else:\n",
    "        compuations['mlp_gate'] = 0\n",
    "    # multiply layer_num\n",
    "    for k, v in compuations.items():\n",
    "        compuations[k] = v * layer_num\n",
    "\n",
    "    # add lm_head\n",
    "    compuations['lm_head'] = batch_size * 2 * seq_len * hidden_size * vocab_size\n",
    "    # backward pass is two times of forward pass\n",
    "    for k, v in compuations.items():\n",
    "        compuations[k] = 3 * v\n",
    "    compuations['total'] = sum(compuations.values())\n",
    "    return compuations\n",
    "\n",
    "# llama2 7B\n",
    "print('llama2-7B ===============>>>>>>>>>>>>>>>')\n",
    "llama2_compuations = analyze_computation(batch_size=1, seq_len=4096, vocab_size=32000, hidden_size=4096, head_num=32, kv_head=32, immediate_size=11008, layer_num=32)\n",
    "for k, v in llama2_compuations.items():\n",
    "    print(f'{k}: {v/1e12: 0.4f} TOPS')\n",
    "    \n",
    "# Qwen2 7B\n",
    "print('Qwe2-7B ===============>>>>>>>>>>>>>>>')\n",
    "qwen2_compuations = analyze_computation(batch_size=1, seq_len=4096, vocab_size=152064, hidden_size=3584, head_num=28, kv_head=4, immediate_size=18944, layer_num=28)\n",
    "for k, v in qwen2_compuations.items():\n",
    "    print(f'{k}: {v/1e12: 0.4f} TOPS')\n",
    "\n",
    "# Qwen2 7B\n",
    "print('Qwe2-7B ===============>>>>>>>>>>>>>>>')\n",
    "qwen2_compuations = analyze_computation(batch_size=2, seq_len=512, vocab_size=152064, hidden_size=3584, head_num=28, kv_head=4, immediate_size=18944, layer_num=28)\n",
    "for k, v in qwen2_compuations.items():\n",
    "    print(f'{k}: {v/1e12: 0.4f} TOPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57344"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7*4096*2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
