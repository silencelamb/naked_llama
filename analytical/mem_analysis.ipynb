{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 1,
>>>>>>> 2f9c9ea488b5b92fdd09395973a6036030fe94de
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama2 7B activations, seq_len=512 ==============>\n",
      "llama2 70B activations==============>\n",
      "Qwen2 7B activations==============>\n",
      "Qwen2 7B activations==============>\n",
      "Qwen2 7B activations==============>\n",
      "head_num=28, kv_head=4, batch_size=1, seq_len=2048, hidden_size=3584, layer_num=28\n",
      "One Transformer Block:\n",
      "QKV input: 0.013671875 GB\n",
      "Q_proj: 0.013671875 GB\n",
      "K_proj: 0.001953125 GB\n",
      "V_proj: 0.001953125 GB\n",
      "O_proj: 0.013671875 GB\n",
      "Softmax output: 0.4375 GB\n",
      "RMSNorm input: 0.0546875 GB\n",
      "MLP up input: 0.013671875 GB\n",
      "MLP up output: 0.072265625 GB\n",
      "MLP gate output: 0.072265625 GB\n",
      "MLP down input: 0.072265625 GB\n",
      "Activation size of One Transformer Block: 0.767578125 GB\n",
      "Total Activation size: 22.11328125 GB\n",
      "Qwen2 7B activations==============>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68.7265625"
      ]
     },
<<<<<<< HEAD
     "execution_count": 4,
=======
     "execution_count": 1,
>>>>>>> 2f9c9ea488b5b92fdd09395973a6036030fe94de
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing\n",
    "from memory import llama_activation\n",
    "GB = 2**30\n",
    "TOPS = 1e12\n",
    "C2C = 200/8  * GB  # per s\n",
    "\n",
    "llama_activation(head_num=32, kv_head=32, batch_size=1, seq_len=4096, hidden_size=4096, \\\n",
    "        immediate_size=11008, layer_num=32, vocab_size=32000, is_print=False)\n",
    "print('llama2 7B activations, seq_len=512 ==============>')\n",
    "llama_activation(head_num=32, kv_head=32, batch_size=1, seq_len=512, hidden_size=4096, \\\n",
    "    immediate_size=11008, layer_num=32, vocab_size=32000, is_print=False)\n",
    "print('llama2 70B activations==============>')\n",
    "llama_activation(head_num=64, kv_head=8, batch_size=1, seq_len=4096, hidden_size=8192, \\\n",
    "    immediate_size=28672, layer_num=80, vocab_size=32000, is_print=False)\n",
    "print('Qwen2 7B activations==============>')\n",
    "llama_activation(head_num=28, kv_head=4, batch_size=256, seq_len=512, hidden_size=3584, \\\n",
    "    immediate_size=18944, layer_num=28, vocab_size=152064, is_print=False)\n",
    "\n",
    "print('Qwen2 7B activations==============>')\n",
    "llama_activation(head_num=28, kv_head=4, batch_size=1, seq_len=1024, hidden_size=3584, \\\n",
    "    immediate_size=18944, layer_num=28, vocab_size=152064, is_print=False)\n",
    "\n",
    "print('Qwen2 7B activations==============>')\n",
    "llama_activation(head_num=28, kv_head=4, batch_size=1, seq_len=2048, hidden_size=3584, \\\n",
    "    immediate_size=18944, layer_num=28, vocab_size=152064)\n",
    "print('Qwen2 7B activations==============>')\n",
    "llama_activation(head_num=28, kv_head=4, batch_size=1, seq_len=4096, hidden_size=3584, \\\n",
    "    immediate_size=18944, layer_num=28, vocab_size=152064, is_print=False)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 2,
>>>>>>> 2f9c9ea488b5b92fdd09395973a6036030fe94de
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.0"
      ]
     },
<<<<<<< HEAD
     "execution_count": 5,
=======
     "execution_count": 2,
>>>>>>> 2f9c9ea488b5b92fdd09395973a6036030fe94de
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def megatron_version(a, b, s, h, L, t=1):\n",
    "    \"\"\"\n",
    "    \"Reducing Activation Recomputation in Large Transformer Models\"\n",
    "    from http://arxiv.org/abs/2205.05198  Page 5. formula (2)\n",
    "    \"\"\"\n",
    "    bytes = s*b*h*L*(10 + 24/t + 5*a*s/h/t)\n",
    "    return bytes/GB\n",
    "\n",
    "megatron_version(a=32, b=1, s=4096, h=4096, L=32, t=1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 34,
>>>>>>> 2f9c9ea488b5b92fdd09395973a6036030fe94de
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2 7B==============>\n",
      "head_num=28, kv_head=4, batch_size=1, seq_len=2048, hidden_size=3584, layer_num=28\n",
      "One Transformer Block:\n",
      "QKV input: 0.013671875 GB\n",
      "Q_proj: 0.013671875 GB\n",
      "K_proj: 0.001953125 GB\n",
      "V_proj: 0.001953125 GB\n",
      "O_proj: 0.013671875 GB\n",
      "Softmax output: 0.4375 GB\n",
      "RMSNorm input: 0.0546875 GB\n",
      "MLP up input: 0.013671875 GB\n",
      "MLP up output: 0.072265625 GB\n",
      "MLP gate output: 0.072265625 GB\n",
      "MLP down input: 0.072265625 GB\n",
      "Activation size of One Transformer Block: 0.767578125 GB\n",
      "Total Activation size: 22.11328125 GB\n",
      "Weights: 14.193356037139893 GB\n",
<<<<<<< HEAD
      "Activations: 22.11328125 GB\n",
      "Grad: 0.0751953125 GB\n",
      "Lora Weights: 0.0751953125 GB\n",
      "Optimizer: 0.451171875 GB\n",
      "Total: 36.90819978713989 GB\n"
=======
      "Activations: 40.03125 GB\n",
      "Grad: 77.0 MB\n",
      "Lora Weights: 77.0 MB\n",
      "Optimizer: 0.451171875 GB\n",
      "Total: 54.82616853713989 GB\n"
>>>>>>> 2f9c9ea488b5b92fdd09395973a6036030fe94de
     ]
    }
   ],
   "source": [
    "print('Qwen2 7B==============>')\n",
    "params_num = 7.62 * 10**9\n",
    "weights = 2 * params_num / GB\n",
    "activations = llama_activation(head_num=28, kv_head=4, batch_size=1, seq_len=2048, hidden_size=3584, \\\n",
    "    immediate_size=18944, layer_num=28, vocab_size=152064)\n",
    "\n",
    "lora_r = 64\n",
    "hidden_size = 3584\n",
    "head_num = 28\n",
    "kv_head = 4\n",
    "immediate_size = 18944\n",
    "layer_num = 28\n",
    "head_num =28\n",
    "kv_head = 4\n",
    "lora_A_q_k_v_o = hidden_size * lora_r * 2\n",
    "lora_B_q_o =  lora_r * hidden_size * 2\n",
    "lora_B_k_v = lora_r * hidden_size/head_num*kv_head * 2\n",
    "lora_A_up_gate = hidden_size * lora_r * 2\n",
    "lora_B_up_gate = lora_r * immediate_size * 2\n",
    "lora_A_down = immediate_size * lora_r * 2\n",
    "lora_B_down = lora_r * hidden_size * 2\n",
    "# lora_weights = layer_num*(4 * lora_A_q_k_v_o + 2 * lora_B_q_o + 2 * lora_B_k_v + 3 * lora_A_up_gate + 3 * lora_B_up_gate)/GB\n",
<<<<<<< HEAD
    "lora_weights = layer_num*(4 * lora_A_q_k_v_o + 2 * lora_B_q_o + 2 * lora_B_k_v)/GB\n",
=======
    "# 最终算法选定的版本，只lora QKVO，没有MLP\n",
    "lora_weights = layer_num*(4 * lora_A_q_k_v_o + 2 * lora_B_q_o + 2 * lora_B_k_v) / GB\n",
>>>>>>> 2f9c9ea488b5b92fdd09395973a6036030fe94de
    "grad = lora_weights\n",
    "optimizer = lora_weights/2*12\n",
    "print(f'Weights: {weights} GB')\n",
    "print(f'Activations: {activations} GB')\n",
    "print(f'Grad: {grad*1024} MB')\n",
    "print(f'Lora Weights: {lora_weights*1024} MB')\n",
    "print(f'Optimizer: {optimizer} GB')\n",
    "print(f'Total: {weights + activations + grad + lora_weights + optimizer} GB') \n",
    "\n",
    "# batchsize = 1->2; 28GB -> 34GB  实测\n",
    "# 脚本，却是 19->21GB,只增加2.5GB\n",
    "\n",
    "# 用了flash att是 58GB\n",
    "# SPDA 是 60多GB\n",
    "# 不用 是 OutOfMem\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "length = 512\n",
    "\n",
    "512*3584  3584,3584\n",
    "\n",
    "\n",
    "4096*4096*2  = 32MB\n",
    "32/2 = 16 MB\n",
    "\n",
    "4096, 2048，  4096， 128\n",
    "\n",
    "3584/2 = 1792\n",
    "\n",
    "3584, 1792   3584, 112\n",
    "\n",
    "\n",
    "计算量统计\n",
    "读写量统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1811\n",
      "0.02511\n",
      "batchsize = 1 >>>>>>>>>>>>>>>\n",
      "TP, Comm bytes: 0.0625 GB\n",
      "TP, Comm bytes total: 2.0 GB\n",
      "TP, Comm time: 0.08 s\n",
      "batchsize = 4 >>>>>>>>>>>>>>>\n",
      "TP, Comm bytes: 0.25 GB\n",
      "TP, Comm bytes total: 8.0 GB\n",
      "TP, Comm time: 0.32 s\n"
     ]
    }
   ],
   "source": [
    "def all_reduce(C2C, bytes, device_num):\n",
    "    comm_time = bytes/device_num/C2C * (device_num-1) * 2\n",
    "    return comm_time\n",
    "\n",
    "b , s, h, immediate_size = 1, 4096, 4096, 11008\n",
    "\n",
    "layer_num = 32\n",
    "tp = 2\n",
    "def TP_comm(b, s, h, layer_num):\n",
    "    print(f'batchsize = {b} >>>>>>>>>>>>>>>')\n",
    "    comm_byte = b * s * h * 2 + b * s * h * 2\n",
    "    print(f\"TP, Comm bytes: {comm_byte/GB} GB\")\n",
    "    comm_byte_total = comm_byte * layer_num\n",
    "    print(f\"TP, Comm bytes total: {comm_byte_total/GB} GB\")\n",
    "    comm_time = all_reduce(C2C, comm_byte_total, tp)\n",
    "    print(f\"TP, Comm time: {comm_time} s\")\n",
    "\n",
    "print(all_reduce(C2C, 7.62*2*GB, 32))\n",
    "print(all_reduce(C2C, 0.162*2*GB, 32))\n",
    "TP_comm(b, s, h, layer_num)\n",
    "TP_comm(4, s, h, layer_num)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP Comm bytes: 7.096678018569946 GB\n",
      "DP Comm time: 0.532250851392746 s\n"
     ]
    }
   ],
   "source": [
    "params_num = 7.62 * 10**9\n",
    "weights = 2 * params_num\n",
    "tp = 2\n",
    "total_devcie = 32\n",
    "comm_byte = weights/tp\n",
    "print(f\"DP Comm bytes: {comm_byte/GB} GB\")\n",
    "comm_time = all_reduce(C2C, comm_byte, total_devcie/tp)\n",
    "print(f\"DP Comm time: {comm_time} s\")\n",
    "\n",
    "#DP 时间不会随着batchszie变，但是会随着tp变\n",
    "# 所以增加tp，减少DP时间，同时增大batchsize，是个不错的选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute time: 0.06096 s\n",
      "Communication: 0.19140625 GB \n",
      "Communication time: 0.0153125 s\n",
      "3.981061224489796\n",
      "3.657142857142857\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_len = 512\n",
    "layer_num = 28\n",
    "hidden_size = 3584\n",
    "Compute = batch_size * seq_len * 2*params_num\n",
    "\n",
    "Compute_time = Compute/128/TOPS\n",
    "print(f\"Compute time: {Compute_time} s\")\n",
    "Communication = batch_size * seq_len * hidden_size * 2 * 2 * layer_num\n",
    "Communication_time = Communication/C2C * 2\n",
    "print(f\"Communication: {Communication/GB} GB \")\n",
    "\n",
    "print(f\"Communication time: {Communication_time} s\")\n",
    "print(Compute_time/Communication_time)\n",
    "print(7*GB/4/hidden_size/layer_num*25/128/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora, DP Comm bytes: 0.3212890625 GB\n",
      "lora, DP Comm time: 0.02489990234375 s\n"
     ]
    }
   ],
   "source": [
    "print(f\"lora, DP Comm bytes: {lora_weights} GB\")\n",
    "comm_time = all_reduce(C2C, lora_weights*GB, total_devcie)\n",
    "print(f\"lora, DP Comm time: {comm_time} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total compute: 31.21152 TOPS\n",
      "Compute time: 0.24384 s\n",
      "batchsize = 4 >>>>>>>>>>>>>>>\n",
      "TP, Comm bytes: 0.03125 GB\n",
      "TP, Comm bytes total: 0.875 GB\n",
      "TP, Comm time: 0.035 s\n"
     ]
    }
   ],
   "source": [
    "compute_per_token = 2 * params_num\n",
    "s = 512\n",
    "b  = 4\n",
    "total_compute = compute_per_token * s * b\n",
    "hw_compute = 128 * TOPS\n",
    "utilization = 1\n",
    "print(f\"Total compute: {total_compute/TOPS} TOPS\")\n",
    "print(f\"Compute time: {total_compute/hw_compute/utilization} s\")\n",
    "\n",
    "TP_comm(b, s, h, layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total compute: 62.42304 TOPS\n",
      "Compute time: 0.48768 s\n"
     ]
    }
   ],
   "source": [
    "s = 4096\n",
    "total_compute = compute_per_token * s\n",
    "print(f\"Total compute: {total_compute/TOPS} TOPS\")\n",
    "print(f\"Compute time: {total_compute/hw_compute} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算die\n",
    "# 24*32\n",
    "\n",
    "56*2*256/8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409.6\n"
     ]
    }
   ],
   "source": [
    "m, k, n = 512, 4096, 4096\n",
    "\n",
    "operation_intensity = 2 * m * n * k / (2 * m * n + 2* m * k + 2*k * n)\n",
    "print(operation_intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama2-7B ===============>>>>>>>>>>>>>>>\n",
      "q_proj:  13.1941 TOPS\n",
      "k_proj:  13.1941 TOPS\n",
      "v_proj:  13.1941 TOPS\n",
      "att_q_k:  13.1941 TOPS\n",
      "att_v:  13.1941 TOPS\n",
      "o_proj:  13.1941 TOPS\n",
      "mlp_up:  35.4592 TOPS\n",
      "mlp_down:  35.4592 TOPS\n",
      "mlp_gate:  35.4592 TOPS\n",
      "lm_head:  3.2212 TOPS\n",
      "total:  188.7638 TOPS\n",
      "Qwe2-7B ===============>>>>>>>>>>>>>>>\n",
      "q_proj:  8.8390 TOPS\n",
      "k_proj:  1.2627 TOPS\n",
      "v_proj:  1.2627 TOPS\n",
      "att_q_k:  10.1018 TOPS\n",
      "att_v:  10.1018 TOPS\n",
      "o_proj:  8.8390 TOPS\n",
      "mlp_up:  46.7207 TOPS\n",
      "mlp_down:  46.7207 TOPS\n",
      "mlp_gate:  46.7207 TOPS\n",
      "lm_head:  13.3939 TOPS\n",
      "total:  193.9629 TOPS\n",
      "Qwe2-7B ===============>>>>>>>>>>>>>>>\n",
      "q_proj:  2.2098 TOPS\n",
      "k_proj:  0.3157 TOPS\n",
      "v_proj:  0.3157 TOPS\n",
      "att_q_k:  0.3157 TOPS\n",
      "att_v:  0.3157 TOPS\n",
      "o_proj:  2.2098 TOPS\n",
      "mlp_up:  11.6802 TOPS\n",
      "mlp_down:  11.6802 TOPS\n",
      "mlp_gate:  11.6802 TOPS\n",
      "lm_head:  3.3485 TOPS\n",
      "total:  44.0712 TOPS\n"
     ]
    }
   ],
   "source": [
    "def analyze_computation(batch_size, seq_len, vocab_size, hidden_size, head_num, kv_head, immediate_size, layer_num, mlp_style='llama'):\n",
    "    '''\n",
    "    count and analyze computations of LLM model training\n",
    "    Reference:\n",
    "    'Efficient large-scale language model training on GPU clusters using megatron-LM' \n",
    "    https://dl.acm.org/doi/10.1145/3458817.3476209, page11, APPENDIX: FLOATING-POINT OPERATIONS\n",
    "    \n",
    "    Parameters:\n",
    "    - batch_size (int): The batch size.\n",
    "    - seq_len (int): The sequence length.\n",
    "    - vocab_size (int): The vocabulary size.\n",
    "    - hidden_size (int): The hidden size.\n",
    "    - head_num (int): The number of attention heads.\n",
    "    - kv_head (int): The number of key-value heads.\n",
    "    - immediate_size (int): The immediate size, ffn up dim.\n",
    "    - layer_num (int): The number of layers.\n",
    "    - mlp_style (str): The style of mlp, 'llama' or 'normal'.\n",
    "\n",
    "    Returns:\n",
    "    - int: The total computations (FLOPs).\n",
    "    '''\n",
    "    compuations = {}\n",
    "    # forward pass\n",
    "    compuations['q_proj'] = batch_size * 2 * seq_len * hidden_size * hidden_size\n",
    "    compuations['k_proj'] = batch_size * 2 * seq_len * hidden_size * (hidden_size // head_num * kv_head)\n",
    "    compuations['v_proj'] = batch_size * 2 * seq_len * hidden_size * (hidden_size // head_num * kv_head)\n",
    "    compuations['att_q_k'] = batch_size * 2 *  seq_len * seq_len * hidden_size\n",
    "    compuations['att_v'] = batch_size * 2 * seq_len * seq_len * hidden_size\n",
    "    compuations['o_proj'] = batch_size * 2 * seq_len * hidden_size * hidden_size\n",
    "    compuations['mlp_up'] = batch_size * 2 * seq_len * hidden_size * immediate_size\n",
    "    compuations['mlp_down'] = batch_size * 2 * seq_len * immediate_size * hidden_size\n",
    "    \n",
    "    if mlp_style == 'llama':        \n",
    "        compuations['mlp_gate'] = batch_size * 2 * seq_len * hidden_size * immediate_size\n",
    "    else:\n",
    "        compuations['mlp_gate'] = 0\n",
    "    # multiply layer_num\n",
    "    for k, v in compuations.items():\n",
    "        compuations[k] = v * layer_num\n",
    "\n",
    "    # add lm_head\n",
    "    compuations['lm_head'] = batch_size * 2 * seq_len * hidden_size * vocab_size\n",
    "    # backward pass is two times of forward pass\n",
    "    for k, v in compuations.items():\n",
    "        compuations[k] = 3 * v\n",
    "    compuations['total'] = sum(compuations.values())\n",
    "    return compuations\n",
    "\n",
    "# llama2 7B\n",
    "print('llama2-7B ===============>>>>>>>>>>>>>>>')\n",
    "llama2_compuations = analyze_computation(batch_size=1, seq_len=4096, vocab_size=32000, hidden_size=4096, head_num=32, kv_head=32, immediate_size=11008, layer_num=32)\n",
    "for k, v in llama2_compuations.items():\n",
    "    print(f'{k}: {v/1e12: 0.4f} TOPS')\n",
    "    \n",
    "# Qwen2 7B\n",
    "print('Qwe2-7B ===============>>>>>>>>>>>>>>>')\n",
    "qwen2_compuations = analyze_computation(batch_size=1, seq_len=4096, vocab_size=152064, hidden_size=3584, head_num=28, kv_head=4, immediate_size=18944, layer_num=28)\n",
    "for k, v in qwen2_compuations.items():\n",
    "    print(f'{k}: {v/1e12: 0.4f} TOPS')\n",
    "\n",
    "# Qwen2 7B\n",
    "print('Qwe2-7B ===============>>>>>>>>>>>>>>>')\n",
    "qwen2_compuations = analyze_computation(batch_size=2, seq_len=512, vocab_size=152064, hidden_size=3584, head_num=28, kv_head=4, immediate_size=18944, layer_num=28)\n",
    "for k, v in qwen2_compuations.items():\n",
    "    print(f'{k}: {v/1e12: 0.4f} TOPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57344"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7*4096*2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
