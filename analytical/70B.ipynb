{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation memory: 1134.31GB\n",
      "Optimizer memory: 782.31GB\n",
      "Grad memory: 130.39GB\n",
      "Weights memory: 130.39GB\n",
      "Total memory need: 2177.39GB\n",
      "Total 3DDRAM Memory: 2560.00GB\n",
      "computation_time: 0.2489s\n",
      "communication: 160.00GB\n",
      "one_time_comm: 1.0000GB\n",
      "communication_time: 0.8000s\n",
      "compute/communication ratio: 0.31\n",
      "one_comutation: 0.002147484TFlops\n",
      "one_comutation_time: 0.003728270 ms\n",
      "one_chunk: 0.015625000GB\n",
      "one_chunk_time: 0.039062500 ms\n"
     ]
    }
   ],
   "source": [
    "from memory import llama_activation\n",
    "\n",
    "GB = 2**30\n",
    "Tflops = 1e12\n",
    "Billion = 1e9\n",
    "\"\"\" hardware parameters \"\"\"\n",
    "# 3ddram\n",
    "dram3d_capacity = 40 * GB\n",
    "card_compute = 576 * Tflops\n",
    "c2c_bandwidth = 400 * GB\n",
    "\n",
    "# 按照H100的设置\n",
    "# dram3d_capacity = 80 * GB\n",
    "# card_compute = 1000 * Tflops\n",
    "# c2c_bandwidth = 450 * GB\n",
    "\n",
    "\n",
    "\"\"\"model config\"\"\"\n",
    "llama_config = {\n",
    "    70 * Billion: {\n",
    "        \"hidden_size\": 8192,\n",
    "        \"layer_num\": 80,\n",
    "        \"head_num\": 64,\n",
    "        \"kv_head\": 8,\n",
    "        \"immediate_size\": 12288,\n",
    "        \"vocab_size\": 128256,\n",
    "\n",
    "\n",
    "    },\n",
    "    8 * Billion: {\n",
    "        \"hidden_size\": 4096,\n",
    "        \"layer_num\": 32,\n",
    "        \"head_num\": 64,\n",
    "        \"kv_head\": 8,\n",
    "        \"immediate_size\": 6144,\n",
    "        \"vocab_size\": 128256,\n",
    "    },\n",
    "}\n",
    "\"\"\"common config\"\"\"\n",
    "# params = 8 * Billion\n",
    "# seq_len = 4096\n",
    "# batch_size = 8\n",
    "# card_num = 8\n",
    "\n",
    "params = 70 * Billion\n",
    "seq_len = 8192\n",
    "batch_size = 8\n",
    "card_num = 64\n",
    "\n",
    "\"\"\"get model config\"\"\"\n",
    "\n",
    "hidden_size = llama_config[params][\"hidden_size\"]\n",
    "layer_num = llama_config[params][\"layer_num\"]\n",
    "head_num = llama_config[params][\"head_num\"]\n",
    "kv_head = llama_config[params][\"kv_head\"]\n",
    "immediate_size = llama_config[params][\"immediate_size\"]\n",
    "vocab_size = llama_config[params][\"vocab_size\"]\n",
    "\n",
    "activation_mem = llama_activation(head_num=head_num, kv_head=kv_head, batch_size=batch_size, seq_len=seq_len, \\\n",
    "    hidden_size=hidden_size, immediate_size=immediate_size, layer_num=layer_num, vocab_size=vocab_size, is_print=False, use_flash_attention=True)\n",
    "activation_mem = activation_mem * GB\n",
    "\n",
    "optimizer_mem = 12 * params  # FP32 weights, FP32 momentum, FP32 2nd momentum\n",
    "grad_mem = 2 * params\n",
    "weights_mem = 2 * params\n",
    "\n",
    "total_mem = activation_mem + optimizer_mem + grad_mem + weights_mem\n",
    "print(f\"Activation memory: {activation_mem/GB:.2f}GB\")\n",
    "print(f\"Optimizer memory: {optimizer_mem/GB:.2f}GB\")\n",
    "print(f\"Grad memory: {grad_mem/GB:.2f}GB\")\n",
    "print(f\"Weights memory: {weights_mem/GB:.2f}GB\")\n",
    "print(f\"Total memory need: {total_mem/GB:.2f}GB\")\n",
    "\n",
    "print(f\"Total 3DDRAM Memory: {dram3d_capacity*card_num/GB:.2f}GB\")\n",
    "\n",
    "computation = batch_size * seq_len * 2 * params\n",
    "computation_time = computation / card_compute / card_num\n",
    "print(f\"computation_time: {computation_time:.4f}s\")\n",
    "\n",
    "communication = batch_size * seq_len * hidden_size * 2 * 2 * layer_num  # BF16=2Byte, 2 All-reduce/layer\n",
    "print(f\"communication: {communication/GB:.2f}GB\")\n",
    "one_time_comm = batch_size * seq_len * hidden_size * 2\n",
    "print(f\"one_time_comm: {one_time_comm/GB:.4f}GB\")\n",
    "\n",
    "communication_time = communication / c2c_bandwidth * 2\n",
    "\n",
    "print(f\"communication_time: {communication_time:.4f}s\")\n",
    "\n",
    "print(f\"compute/communication ratio: {computation_time/communication_time:.2f}\")\n",
    "\n",
    "one_comutation = batch_size * (seq_len/card_num) * hidden_size  * (hidden_size/card_num) * 2\n",
    "print(f\"one_comutation: {one_comutation/Tflops:.9f}TFlops\")\n",
    "one_comutation_time = one_comutation / card_compute\n",
    "print(f\"one_comutation_time: {one_comutation_time*1000:.9f} ms\")\n",
    "\n",
    "one_chunk = batch_size * (seq_len/card_num) * hidden_size * 2\n",
    "print(f\"one_chunk: {one_chunk/GB:.9f}GB\")\n",
    "one_chunk_time = one_chunk / c2c_bandwidth\n",
    "print(f\"one_chunk_time: {one_chunk_time*1000:.9f} ms\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
